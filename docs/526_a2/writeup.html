<html>
<head>
	<title>RGBD Image Composition Writeup, COS 526</title>
	<style>
		body, figure
		{
			margin:.5em auto;
			max-width:60em;
			padding:0 .62em;
			font: 1em/1.2em sans-serif; 
			color:#444;
		}

		section
		{
			padding-top:1em;
		}

		h1,h2,h3
		{
			line-height:1.1em;
		}

		h3
		{
			padding-top:1em;
		}

		img
		{
			display:block;
			width:100%;
		}

		td 
		{
			align:center;
		}
	</style>
</head>
<body>
	<section>
		<center>
			<h1>
				Writeup for COS 526 Assignment 2: RGBD Image Composition
			</h1>
			<h3> Crystal Qian, 11.06.16</h3>
		</center>
	</section>
	<hr>
	<section>
		<p> This assignment implements an algorithm for composing multiple images into a single texture. Here are some features of my project:
			<ol>
				<li> <b>Image selection</b>: We choosing the <i>k</i> best images per texel based on "quality of observation" and also choosing the <i>k</i> best images per surface.</li>
				<li> <b>Texture color estimation</b>: The color at each texel is calculated as either the weighted mean or weighted median of pixels in selected images based on "quality of observation."</li>
				<li> <b>Surface creation</b>: We create rectangular surfaces automatically by detecting planes in the input point clouds.</li>
			</ol> 
		</p>
		<p>
			Modifications were made to <code>texture.cpp</code>, <code>render.h</code>, and mostly <code>render.cpp</code>. Note about the directory structure: the assignment page said to submit the input/ directory, but also to not submit the input/ directory, so I didn't submit the input/ directory.
		</p>
	</section>
	<hr>
	<section>
		<h2 id="quality"> Quality of observation </h2>
		<p> The quality of an image at each pixel is dependent on the bluriness of the image <i>B</i>, the viewing angle of the image <i>theta</i>, and the viewing distance of the particular pixel <i>d</i>. Specifically, the quality is proportional to <code>(cos(<i>theta</i>) * <i>B</i>) / <i>d</i> </code>, similar to [Maier16]. In the code, this is done in the <code>GetPixelWeight()</code> method.</p>

		<h4 id="blur"> Bluriness of image </h4>
		<p> Each image is associated with a bluriness value; the blurrier the image, the higher this value. We use a <a href="https://infoscience.epfl.ch/record/111802/files/14%20A%20no-reference%20perceptual%20blur%20metric.pdf">no-reference perceptual blur metric</a> to calculate this. </p>
		<ol>
			<li> First, we do a edge detect on our image by convoluting its grayscale values with a vertical Sobel operator <code>[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]</code>. If the edge detect value at a pixel is above a certain threshold (in our case, .01), it is an edge pixel.
				<table>
					<tr>
						<td>	
							<figure>
								<img src="output/vertical_edge_detect.jpg" style="width:50%"/> 
								<figcaption>Vertical edge detection. This was taken before I realized we could toggle the display of the point map and camera.</figcaption>
							</figure>
						</td>
					</tr>
				</table>
			</li>
			<li> Because we used only a vertical Sobel operator, we then iterate across each row of the image and store each "edge width," which is the number of continuous edge pixels.</li>
			<li> The average of these edge widths is the blur value of the image.</li>
		</ol>
		<h4> Viewing angle </h4>
		<p> The viewing angle is the dot product of the <i>WorldNormal()</i> of the surface and the <i>WorldTowards()</i> of the image. </p>
		<h4> Viewing distance </h4>
		<p> This value, <i>d</i>, is the pixel's depth channel. In [Maier16], the quality metric is divided by <i>d</i> * <i>d</i>. Although this causes some undesired edges in the resultant image as opposed to when dividing by only <i>d</i>, I think the increased sharpness looks nicer.</p>
		<table>
			<tr>
				<td>	
					<figure>
						<img src="output/median_nod.jpg"/> 
						<figcaption>Resultant median when depth is not a factor of quality.</figcaption>
					</figure>
				</td>
				<td>	
					<figure>
						<img src="output/median_dnotd2.jpg"/> 
						<figcaption>Resultant median when dividing by only <i>d</i>.</figcaption>
					</figure>
				</td>
				<td>	
					<figure>
						<img src="output/median.jpg"/> 
						<figcaption>Resultant median when dividing by <i>d</i> * <i>d</i>.</figcaption>
					</figure>
				</td>
			</tr>
		</table>
	</section>
	<hr>
	<section>
		<h2> Image selection </h2>
		<p> Instead of taking in account all images that map to a surface and also all images with pixels corresponding to a texel, we can just take in the "best" images. To apply this in our program, add the argument <code>-choose_k_texel <i>k</i></code> to specify how many images <i>k</i> should be considered per texel, and the argument <code>-choose_k_surface <i>k</i></code> to specify how many images <i>k</i> should be considered per surface. These two selection processes can be run on the same texture generation; first, we prune the number of images used per surface. Then, the remaining images are used to calculate the best images per texel.</p>

		<h3> Selection per texel </h3>
		<p> Image selection per texel is based on the average <a href="#quality">quality of observation</a> of the pixels of each image that map to that particular texel. If we choose a value <i>k</i> that is larger than our number of images, all images will be used. If not, for each texel, we choose <i>k</i> images with the highest qualities of observation.</p>

		<table>
			<tr>
				<td>	
					<figure>
						<img src="output/median_10texel.jpg"/> 
						<figcaption>-choose_k_texel = 10.</figcaption>
					</figure>
				</td>
				<td>	
					<figure>
						<img src="output/median_5texel.jpg"/> 
						<figcaption>-choose_k_texel = 5.</figcaption>
					</figure>
				</td>
				<td>	
					<figure>
						<img src="output/median_1texel.jpg"/> 
						<figcaption>-choose_k_texel = 1.</figcaption>
					</figure>
				</td>
			</tr>
		</table>

		<p> The text in the yellow boxes shows the distinction between <i>k</i> texels pretty well.</p>

		<h3> Selection per surface (optional feature) </h3>
		<p> Originally, I implemented the selection per surface algorithm exactly as described in [jeon16]. There were two passes: first, a bluriness pass to choose the sharpest out of <i>maxSigma</i> images, then skip <i>minSigma</i>, then, a uniqueness pass where images that remained had the most depth differences. </p>

		<p> This doesn't work for us, though, unless we have very precise surface detection. For example, take a wall with chairs represented as one surface. This algorithm will give us mainly just chairs because of all the varying depths. At least, that's why my implementation was yielding. </p>

		<p> So, here's a new algorithm I'm proposing that works a lot better for our cases. </p>
		<ol>
			<li>First, map each texel to a list of images that have pixels which cover the texel.</li>
			<li>Then, while we have more than <i>k</i> images per surface ... </li>
			<li>We give each image a score; the summed score of each pixel based on weight of observation (just like in image selection per texel). However, the score is also weighted by uniqueness. If the pixel is the only pixel representing a texel, it is rewarded by a much higher score.
			</li>
			<li>We remove the image with the least score and update the texel map.</li>
			<li>This repeats until we have <i>k</i> images remaining for the surface. <b>Note: if the user removes too many images, this will look bad for obvious reasons (eventually there will be holes).</b></li>
		</ol>
		<table>
			<tr>
				<td>	
					<figure>
						<img src="output/unique150faster.jpg" style="width:50%"/> 
						<figcaption>We assign a higher score if the pixel is the only one representing the texel.</figcaption>
					</figure>
				</td>
			</tr>
			<tr>
				<td>
					<figure>
						<img src="output/unique150faster-weighted.jpg" style="width:50%"/> 
						<figcaption>As opposed to in this image, where we assign a score scaled by the number of other pixels representing the texel. I find this result worse.</figcaption>
					</figure>
				</td>
			</tr>
		</table>
		<p>We borrowed the optimization from [jeon16] of sampling a grid, though, rather than the entire image for speed reasons. Our current grid size is 20 x 20. At lower values of <i>kPerSurface</i>, you can tell a slight difference:</p>
		<table>

			<tr>
				<td>	
					<figure>
						<img src="output/unique150.jpg"/> 
						<figcaption>kPerSurface = 150, using all pixels (no grid).</figcaption>
					</figure>
				</td>
				<td>
					<figure>
						<img src="output/unique150faster.jpg"/> 
						<figcaption>kPerSurface = 150, using a 20 x 20 grid.</figcaption>
					</figure>
				</td>
				<td>
					<figure>
						<img src="output/unique150fast.jpg"/> 
						<figcaption>kPerSurface = 150, using a 10 x 10 grid.</figcaption>
					</figure>
				</td>
			</tr>
		</table>
		<p> Here are some results at different values of kPerSurface. </p>
		<table>
			<tr>
				<td>	
					<figure>
						<img src="output/painting_weighted_median.jpg"/> 
						<figcaption>We use all images per surface here..</figcaption>
					</figure>
				</td>
				<td>
					<figure>
						<img src="output/kPerSurface200.jpg"/> 
						<figcaption>kPerSurface = 200.</figcaption>
					</figure>
				</td>
				<td>
					<figure>
						<img src="output/kPerSurface100.jpg"/> 
						<figcaption>kPerSurface = 100.</figcaption>
					</figure>
				</td>
			</tr>
		</table>
	</section>
	<hr>
	<section>
		<h2>Texture color estimation</h2>
		<p>The weights used are based on the <a href="#quality">quality of observation</a> calculations described above. The program <code>texture.exe</code> defaults to calculating weighted median. To calculate weighted mean instead, add the argument <code>-weighted_mean 1</code>.

			<h3>Weighted median</h3>
			To calculate the weighted median color of a texel, we create three maps for each texel (one for each color channel) with the key being the image's color at the texel's position and the value being the weight of this pixel. We also track the total weight of all images. Then, we traverse our sorted maps; the weighted median color is the color at which the cumulative weight is equal to half the total weight.

			<table>
				<tr>
					<td>	
						<figure>
							<img src="output/median_naive.jpg"/> 
							<figcaption>Regular median.</figcaption>
						</figure>
					</td>
					<td>
						<figure>
							<img src="output/median.jpg"/> 
							<figcaption>Weighted median.</figcaption>
						</figure>
					</td>
				</tr>
			</table>

			<h3>Weighted mean</h3>
			To calculate the weighted mean color of a texel, we track three values for each texel (one for each color channel), the sum of <i>color x weight</i> for each image. We also track the total weight. The weighted mean is simply the sum of <i>color x weight</i> divided by the total weight.

			<table>
				<tr>
					<td>	
						<figure>
							<img src="output/mean_naive.jpg"/> 
							<figcaption>All weights set to 1 (regular mean calculation).</figcaption>
						</figure>
					</td>
					<td>
						<figure>
							<img src="output/mean.jpg"/> 
							<figcaption>Weighted mean.</figcaption>
						</figure>
					</td>
				</tr>
			</table>
		</section>
		<section>
			<h2> Surface detection/ creation via RANSAC (optional feature) </h2>
			<p> Implementing this feature was by far the most challenging thing I did during fall break, except for that time where I stuffed <i>twenty-four</i> xiao long bao (soup dumplings) down my gullet at a street vendor's stand in Shanghai. I'm going to talk you through the emotional process that was surface creation.</p>

			<p> Getting started was tricky without a reference point. Getting started was also tricky without fast access to Google (thank you, Chinese firewall!). However, this allowed me to really get creative and come up with the worst solution possible.</p>

			<p> To activate surface creation, just add the argument <code>-detect_surfaces <i>k</i></code>, where <i>k</i> is the number of surfaces you wish to detect. We won't always find <i>k</i> surfaces (for example, if the texture is a wall and you ask for a gazillion surfaces). </p>

			<h3> Approaches that were tried </h3>
			<ul>
				<li> <b>Refining a plane.</b> In this approach, we repeatedly select three points to form a plane. Using a kd-tree to store every <i>n</i>-th point in each image, we find the closest points the the plane within that range, and use a least-squares algorithm to compute a median point and axis. We then iteratively decrease the bounds of the plane (forming a rectangle) until desired density. This approach was slow, but more importantly, calculating axes by least-square didn't realy work for me.</li> 
				<li> <b>Refining a plane part 2.</b> Okay, new idea. After we get the plane, we sweep a vector around in a circle to calculate the "length" of the plane at each direction (calculated by the interquartile range of close points along each axis, and subsequent rectangle-size forming by outliers). This was the most trying time of the process. I wrote a stupid <i>CustomRectangle</i> class for this, and later discovered <code>R3OrientedBox</code>.</li>
				<li> <b>Refining a rectangle.</b> Then, a bright idea came to me. The difficult part was making a rectangle from a plane. Why don't I just RANSAC the rectangle?? </li>
			</ul>
			<h3> Current approach </h3>
			<p> Before I walk though the algorithm, I'm going to show you some pretty pictures so you can see that this actually works sometimes. </p>
			<table>
				<tr>
					<td>
						<figure>
							<img src="output/point_cloud.jpg"/>
							<figcaption>Point cloud depiction of corridor B. Notice areas of larger density of points.</figcaption>
						</figure>
					</td>
					<td>
						<figure>
							<img src="output/point_cloud_side.jpg"/>
							<figcaption>This is the same point cloud from the side. See that mass of points sticking out of the right side?</figcaption>
						</figure>
					</td>
				</tr>
				<tr>
					<td>
						<figure>
							<img src="output/3surface.jpg"/>
							<figcaption>BOOM. 3 surfaces detected.</figcaption>
						</figure>
					</td>
					<td>
						<figure>
							<img src="output/3surface2.jpg"/>
						</figure>
					</td>
				</tr>
				<tr>
					<td>
						<figure>
							<img src="output/4surface.jpg"/>
							<figcaption>4 surfaces detected.</figcaption>
						</figure>
					</td>
					<td>
						<figure>
							<img src="output/4surface2.jpg"/>
						</figure>
					</td>
				</tr>
				<tr>
					<td>
						<figure>
							<img src="output/5surface.jpg"/>
							<figcaption>5 surfaces detected.</figcaption>
						</figure>
					</td>
					<td>
						<figure>
							<img src="output/5surface2.jpg"/>
						</figure>
					</td>
				</tr>
				<tr>
					<td>
						<figure>
							<img src="output/pointmap.jpg"/>
							<figcaption>Point map for the tiled floor.</figcaption>
						</figure>
					</td>
					<td>
						<figure>
							<img src="output/original.jpg"/>
							<figcaption>Original tiled floor (no surface detection).</figcaption>
						</figure>
					</td>
				</tr>
				<tr>
					<td>
						<figure>
							<img src="output/3surfaces.jpg"/>
							<figcaption>3 surfaces detected.</figcaption>
						</figure>
					</td>
					<td>
						<figure>
							<img src="output/10surfaces.jpg"/>
							<figcaption>10 surfaces detected.</figcaption>
						</figure>
					</td>
				</tr>
			</table>
			<p> Here's how we do this.</p>
			<ol>
				<li> First, populate a point cloud vector with all the points in all images of the surface. For optimization purposes, I take every tenth pixel.</li>
				<li> For all existing surfaces in the configuration, I make a R3OrientedBox along the surface, with depth covering a maxDistanceFromPlane value. We remove all points in the point cloud that are in these boxes.</li>
				<li> Then, while we still have a significant amount of points in our point cloud vector or we haven't found the desired number of surfaces... </li>
				<li> We randomly pick two points to bound a rectangle. This rectangle is turned into a R3OrientedBox, with depth maxDistanceFromPlane.</li>
				<li> We add all points in the point cloud vector that intersect this box to a list, and use the list to calculate the density of the box.</li>
				<li> If the density is higher than a fixed density threshold, we check if this box intersects any boxes of surfaces we're already planning on adding. If so, we check if these boxes might possibly be combined to make a bigger rectangle! If not, we punish this box for intersecting by assigning it a lower value. </li>
				<li> The resultant "value" of this box depends on density, size (we reward bigger boxes) and any punishment that might have happened. If this is the best factor thus far and the point map is a signicant size (further punishing realy dense, tiny patches), we set it as our best box. </li>
				<li> After doing this many times (RANSAC), we transform the best box into a surface!</li>
			</ol>

			<h3> Problems with current approach </h3>
			<p> I glossed over the part where we turn two points into a rectangle. I noticed that most environments we're scanning face a plane head-on. Because it's an indoor environment, most planes are linear. That is, their normals are either the x-, y-, or z- axis. I take the axis with the minimum distance between the two points and make that the normal. Taking advantage of this preexisting knowledge of the input makes it more intuitive to do things like combine surfaces, etc. The results look pretty decent, I think, but sometimes...
				<table>
					<tr>
						<td>
							<figure>
								<img src="output/badinitial.jpg"/>
								<figcaption>you turn a perfectly good rendering like this... </figcaption>
							</figure>
						</td>
						<td>
							<figure>
								<img src="output/bad4surfaces.jpg"/>
								<figcaption>Into this by looking for four surfaces.</figcaption>
							</figure>
						</td>
						<td>
							<figure>
								<img src="output/point_cloud_vulnerability.jpg"/>
								<figcaption>Here's the point cloud.</figcaption>
							</figure>
						</td>
					</tr>
				</table>
				<p>The problem with this particular example is it doesn't line up with the axes like we assumed. Notice that our surfaces are roughly the correct shape and in the right position, but tilted badly. A way to get around this is by maybe rotating first? Anyway, just a problem I found with my solution. #letmehelpyougrademe</p>

				<p>I'd like to make the surfaces more continuous with one another, so I think it'd be really cool to add some image composition and seam carving to this.</p>
			</section>
			<hr>
			<section>
				<h2> Other things </h2>
				<h3> Bakeoff </h3>
							<table>
					<tr>
						<td>
							<figure>
								<img src="output/bakeoff_painting.jpg" style="width:50%"/>
								<figcaption><code> -detect_surfaces 10 -choose_k_texel 40 -choose_k_surface 100</code></figcaption>
							</figure>
						</td>
					</tr>
				</table>
				<p>
					Like the last assignment, this took a ton of time. But, it was really cool to watch things come together! I felt like we had a lot more freedom on this one.
				</p>
			</section>
		</body>
		</html>
